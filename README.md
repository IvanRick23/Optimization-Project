# Optimization-algos 🚀
_Un répertoire structuré pour l’exploration et la comparaison des algorithmes d’optimisation_

---

## 🎯 Objectif du projet

Ce projet vise à regrouper, implémenter et comparer différents algorithmes d'optimisation utilisés pour résoudre des problèmes variés : linéaires, non-linéaires, moindres carrés, et basés sur la factorisation matricielle.

Chaque algorithme est classé en fonction du **type de problème qu’il résout**. Des **exemples pratiques**, des **analyses de performance**, et des **visualisations** permettent d'en comprendre le fonctionnement et les usages.

---

## 📂 Structure du dépôt

| Dossier                      | Contenu                                                                 |
|-----------------------------|-------------------------------------------------------------------------|
| `linear_optimization/`      | Algorithmes pour problèmes linéaires (Simplex, méthode graphique, etc.) |
| `nonlinear_optimization/`   | Algorithmes pour problèmes non-linéaires (Gradient, Newton, etc.)       |
| `least_squares/`            | Méthodes de moindres carrés (Gauss-Newton, Levenberg-Marquardt)         |
| `matrix_methods/`           | Factorisations (LU, Cholesky, Trigonalisation, etc.)                    |
| `performance_tests/`        | Scripts pour mesurer précision et rapidité des algorithmes              |
| `utils/`                    | Outils communs (fonctions d’erreurs, visualisations, etc.)              |

---

## 📌 Pourquoi ce projet ?

- ✅ Consolider ses connaissances en optimisation
- 📈 Comparer les performances des méthodes
- 👨‍💻 Construire un **portfolio GitHub impressionnant**
- 🎓 Préparer des entretiens techniques et concours
- 🤖 Appliquer l’optimisation en IA, data science, finance, etc.

---

## 📊 Quelques algorithmes inclus

- **Simplex** : résolution de problèmes linéaires
- **Méthode du gradient** : optimisation de fonctions différentiables
- **Gauss-Newton** : moindres carrés non linéaires
- **LU / Cholesky** : méthodes matricielles d’optimisation
- **Levenberg-Marquardt** : interpolation entre Gauss-Newton et Gradient
- **Newton** : descente avec courbure

> 🛠️ D’autres seront ajoutés au fur et à mesure dans chaque branche du dépôt.

---

## 🚀 Utilisation

Tu peux exécuter n’importe quel algorithme depuis son dossier respectif :

```bash
cd nonlinear_optimization/
python gradient_descent.py
```

---

## 🤝 Contribuer
Si tu veux suggérer d'autres méthodes ou améliorer les tests :

- Fork le projet
- Crée une branche
- Propose un Pull Request

---

## 🔗 Auteur
- Projet conçu par [MasterCoder]
- Profil GitHub : [[lien ici](https://github.com/IvanRick23)]
